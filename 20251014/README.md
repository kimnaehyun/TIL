# 선형회귀
* 지도학습의 가장 기초적인 방법
* 입력과 출력의 선형 관계를 학습

# 단순선형회귀
* 모형: Y = β₀ + β₁X + ε
* 의미: β₀:절편, β₁:기울기, ε:측정오차
* 학습 방법: 최소제곱법 (RSS 최소화)
* 사례: TV 광고비와 매출 관계

# 다중선형회귀
* 모형: Y = β₀ + β₁X₁ + β₂X₂ + ··· + βₚXₚ + ε
* 해석: 다른 변수 고정 시, 특정 Xⱼ가 1증가 -> Y는 평균적으로 βⱼ 변화
* 사례: TV + Radio + Newspaper 광고비와 매출

# 계수 추정과 예측
* RSS 최소화를 통해 계수 추정
* 해: 최소제곱 => 정규방정식 사용
* 결과는 평면(hyperplane)으로 표현

# 선형회귀 주의사항
* 훈련 데이터 기반 계산 -> 검증/테스트셋 필요
* 다중공선성 문제 발생 가능 (변수 간 높은 상관성)
  * 계수 해석 어려움
  * 분산 증가 -> 불안정한 추정

# 분류 문제란?
* 분류 : 정해진 범주(카테고리) 중 하나로 지정하는 것
* 분류 모델의 목표 : 분류 함수 f(x)를 학습하여 입력 X가 속할 범주(또는 그 확률)를 예측

# 분류 문제에서 선형회귀 모델 적용의 한계
* 이진 분류 문제 : 선형회귀는 예측 값이 확률 범위(0~1)를 벗어나 부적합
* 다중 분류 문제 : 정수로 코딩 시 인위적 순서를 암시하기에 부적합

# 로지스틱 회귀(Logistic Regression)는 어떻게 작동할까?
* 로지스틱 함수 모형식은 선형 회귀 모형식과 sigmoid 함수의 결합
* 우도 : "확률함수가 데이터를 얼마나 잘 설명하는지"를 나타낸 지표
* 최대 우도 측정(MLE; Maximum Likelihood Estimation) : 우도가 최대가 되는 모수를 찾는 과정

# 경사 하강법 (Gradient Descent)
* Step 1: 손실함수의 기울기(미분값) 계산
* Step 2: 기울기의 반대 방향으로 파라미터 업데이트
* 학습률(α): 한 번에 이동하는 크기 결정

# Convex 문제와 최적화
* 2차 미분이 항상 양수 -> 아래로 볼록한 형태
* 국소 최솟값 = 전역 최솟값 -> 안정적인 학습 가능

# 확률적 경사 하강법 (SGD) 
* 전체가 아닌 일부(미니배치)데이터로 기울기 계산 -> 계산 비용 감소
* 매 업데이트마다 노이즈가 섞여 국소 최솟값에서 벗어나기 유리